\documentclass{article}
\usepackage{ctex}
\usepackage{amsmath}
\usepackage{cases}


\title{一些算法的简单整理}
\begin{document}
\maketitle
\noindent ---------------------------------------------------------------------------------------------------
决策树类：ID3：计算信息增益：
\begin{eqnarray*}
Gain(D,a) = Ent(D) - \sum_{v=1}^{V}\frac{|D^{v}|}{|D|}Ent(D^{v})
\end{eqnarray*}
其中：以 Y 表示结果集，以 V 表示样本的属性子集（单一），对于散点字段：
\begin{eqnarray*}
&&Ent(D) = -\sum_{k=1}^{|Y|}p_{k}log_{2}p_{k}\\
&&Ent(D^{v}) = -\sum_{k=1}^{|Y^{v}|}p_{k,v}log_{2}p_{k,v}\\
\end{eqnarray*}
而对于区间字段,先计算N个样本的的 N-1 个中点集：
\begin{eqnarray*}
T_{a} = \{\frac{a_{i}+a_{i+1}}{2}|1\leq i\leq N-1\}
\end{eqnarray*}
于是，计算所有属性的信息增益，以获取影响结果，信息增益越大，对样本判定影响越强：
\begin{eqnarray*}
a_{*} = argmax_{a\in A}Gain(D,A)
\end{eqnarray*}
随后分别以这 N-1 个候选点为分界线，以数值大小进行二分类比较，从而以二分类的形式计算信息增益：
\begin{eqnarray*}
Gain(D,a) &=& \max_{t\in T_{a}} Gain(D,a,t)\\
&=&\max_{t\in T_{a}}\big(Ent(D) - \sum_{\lambda\in\{-,+\}}\frac{|D_{t}^{\lambda}|}{|D|}Ent(D_{t}^{\lambda})\big)
\end{eqnarray*}
---------------------------------------------------------------------------------------------------
\noindent 决策树类：C4.5：计算信息增益率：
\begin{eqnarray*}
Gain\_ratio(D, a) = \frac{Gain(D, a)}{IV(a)}
\end{eqnarray*}
其中：
\begin{eqnarray*}
&&Gain(D,a) = Ent(D) - \sum_{v=1}^{V}\frac{|D^{v}|}{|D|}Ent(D^{v})\\
&&IV(a) = - \sum_{v=1}^{V}\frac{|D^{v}|}{|D|}log_{2}\frac{|D^{v}|}{|D|}
\end{eqnarray*}

于是，计算所有属性的信息增益率，以获取影响结果，信息增益率越大，对样本判定影响越强：
\begin{eqnarray*}
a_{*} = argmax_{a\in A}Gain\_ratio(D,a)
\end{eqnarray*}
---------------------------------------------------------------------------------------------------

\noindent 决策树类：CART：计算 Gini 指数：
\begin{eqnarray*}
Gini\_index(D,a) = \sum_{v=1}^{V}\frac{|D^{v}|}{|D|}Gini(D^{v})
\end{eqnarray*}
其中:
\begin{eqnarray*}
Gini(D^{v}) = \sum_{k=1}^{|Y^{v}|}\sum_{k^{'}\ne k}p_{k}p_{k^{'}} = 1 - \sum_{k=1}^{|Y^{v}|}p_{k}^{2}
\end{eqnarray*}

于是，计算所有属性的基尼索引，以获取影响结果，Gini索引越小，对样本判定影响越强：
\begin{eqnarray*}
a_{*} = argmax_{a\in A}Gain\_ratio(D,a)
\end{eqnarray*}
--------------------------------------------------------------------------------------------------

\noindent 决策树类：预剪枝处理：通过简单的概率决定划分属性。\\
--------------------------------------------------------------------------------------------------


\noindent 神经网络类：BP神经网络：基于梯度下降将误差函数优化至最小。\\
\begin{eqnarray*}
Error = \sum_{k=1}^{N} Error_{k}
\end{eqnarray*}
其中：
\begin{eqnarray*}
Error_{k} = \frac{1}{2}\sum_{j=1}^{l}(\hat{y}^{k}_{j}- y^{k}_{j})
\end{eqnarray*}
注意：神经网络的逐层训练函数与学习率（就是梯度下降中的常数组）要根据具体情况而设定，而一般采用 sigmoid 函数：
\begin{eqnarray*}
sigmoid(x) = \frac{1}{1+e^{-x}}
\end{eqnarray*}
使用 sigmoid 函数的一些微分性质,对梯度下降策略下的函数优化进行简化。但是本人更习惯使用牛顿法进行优化。\\
注意：BP神经网络形成的图是一个有向无环图\\
--------------------------------------------------------------------------------------------------


\noindent 神经网络类：卷积神经网络：一般用于图像识别基于梯度下降将误差函数优化至最小：\\
通过滤波函数将一个图像的 rgb 空间的三个矩阵进行优化，通常卷积函数使用高斯滤波，从而形成了一个进行像素处理的图像，可递归处理，形成某图像的高斯金字塔。可利用拉普拉斯金字塔进行图像恢复。\\
注意：图像恢复时，在opencv中，是根据本像素估计其邻域像素值，而不是储存删减像素进行恢复\\
--------------------------------------------------------------------------------------------------

\noindent 神经网络类：递归神经网络：有向无环图中加入了环，某输入层形成递归输入。\\
--------------------------------------------------------------------------------------------------


\noindent 回归分析类：线性回归：\\
这种是最简单的回归了，线性回归的本质是最小二乘法，其线性模型为：
\begin{eqnarray*}
f(x) = w^{T}x + b
\end{eqnarray*}
其中，b 是损失函数\\
当然，在数据量并不是很大的情况下，可以使用最小二乘法，当数据量巨大的时候，最小二乘法的效率会降低很多，于是，我个人比较习惯采用牛顿法，或者梯度下降也是很好的\\
--------------------------------------------------------------------------------------------------\\

\noindent 回归分析类：逻辑回归（对数几率回归）：常用于二分类输出，其模型为：\\
\begin{eqnarray*}
f(x) = \sum_{i=1}^{m}ln(p(y_{i}|x_{i};w,b)), \,\,\,\,\, y_{i}\in \{0,1\}
\end{eqnarray*}
其中，二分类输出的概率为：
\begin{eqnarray*}
&&p(y=1|x) = \frac{e^{w^{t}x+b}}{1+e^{w^{t}x+ b}}\\
&&p(y=0|x) = \frac{1}{1+e^{w^{t}x+ b}}
\end{eqnarray*}
而对于对数回归，则需要使用梯度下降或者牛顿法来进行回归处理\\
--------------------------------------------------------------------------------------------------\\

\noindent 回归分析类：SVM（支持向量） 回归（严格模式）：常用于二分类问题，也可运用多分类问题，以二分类划分不同样本为例，模型为：\\
\begin{eqnarray*}
\min_{w,b}\frac{1}{2}||w||^{2},\,\,\,\,\,\,\, s.t.\,\,\,\,\, y_{i}(w^{T}x_{i}+ b) \ge 1
\end{eqnarray*}
这里，回归模型采用了线性模型，即采用一个超平面来分割不同样本（而不是超曲面）。此时，求极值则采用拉格朗日乘子法进行运算。\\
通常，不会直接使用线性模型，而是将自变量x替换成核函数，即模型为：
\begin{eqnarray*}
f(x) = w^{T}\phi(x) + b
\end{eqnarray*}
KKT条件：即对拉格朗日乘子的一些约束：
\begin{eqnarray*}
&&t_{i} \ge 0\\
&&y_{i}f(x_{i}) - 1 \ge 0\\
&&\alpha_{i}(y_{i}f(x_{i}) - 1) = 0
\end{eqnarray*}
--------------------------------------------------------------------------------------------------\\

\noindent 回归分析类：SVR（支持向量） 回归（松弛模式）：对于SVM的方法，允许产生$\epsilon$的误差，其模型为：\\
\begin{eqnarray*}
&&\min_{w,b,\xi_{i},\hat{\xi}_{i}}\frac{1}{2}||w||^{2} + C\sum_{i=1}^{m}(\xi_{i} + \hat{\xi}_{i}),\,\,\,\,\,\,\,\,s.t.\\
&&-\hat{\xi}_{i} - \epsilon\leq f(x_{i})-y_{i} \leq \epsilon + \xi_{i},\,\,\,\,\, \xi_{i} \ge 0\,\,\,\,\, \hat{\xi}_{i} \ge 0
\end{eqnarray*}
同样，使用拉格朗日乘子法求极值\\
而此时的 KKT 条件为：
\begin{eqnarray*}
&&\alpha_{i} \ge 0\\
&&\mu_{i} \ge 0\\
&&\epsilon_{i} \ge 0\\
&&\mu_{i}\epsilon_{i}=0\\
&&\alpha_{i}(y_{i}f(x_{i}) - 1 + \xi_{i}) = 0\\
&&y_{i}f(x_{i}) - 1+\epsilon_{i} \ge 0\\
\end{eqnarray*}
--------------------------------------------------------------------------------------------------\\

\noindent 朴素贝叶斯：一般用于对样本的先验处理，利用概率论中的贝叶斯公式：
\begin{eqnarray*}
P(A)P(B|A) = P(B)P(A|B)
\end{eqnarray*}
因此，朴素贝叶斯表达式为：
\begin{eqnarray*}
H(x) = argmax_{c\in Y} P(C)\prod_{i=1}^{d}P(x_{i}|c)
\end{eqnarray*}
其中，对于散点字段，有：
\begin{eqnarray*}
P(x_{i}|c) = \frac{|D_{c, x_{i}}|}{|D_{c}|}
\end{eqnarray*}
二对于连续字段（一般考虑概率密度函数），有：
\begin{eqnarray*}
P(x_{i}|c) = \frac{1}{\sqrt{2\pi}\sigma_{c,i}}\exp{(-\frac{(x_{i}-\mu_{c,i})^{2}}{2\sigma_{c,i}^{2}})}
\end{eqnarray*}
--------------------------------------------------------------------------------------------------\\

\noindent 优化算法底层相关：梯度下降：沿梯度下降的方向求极小值，或沿梯度上升的方向求极大值：
迭代公式：
\begin{eqnarray*}
a_{k+1} = a_{k} + \rho_{k}\bar{s}^{k}
\end{eqnarray*}
其中：$\bar{s}^{k}$ 为函数 $f(x)$ 在某一点的梯度反向， $\rho_{k}$ 为一些递减的固定正值，下确界为0.\\
--------------------------------------------------------------------------------------------------\\

\noindent 优化算法底层相关：批量梯度下降（BGD）：使用全局所有样本进行梯度下降策略，优化函数为最小二乘法为基本函数的损失函数\\
迭代公式：
\begin{eqnarray*}
\theta_{j+1} = \theta_{j} - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})- y^{i})x^{(i)}_{j}
\end{eqnarray*}
其中:
\begin{eqnarray*}
h_{\theta}(x^{(i)}) = \sum_{j=1}^{n}\theta_{j}x_{j}^{(i)}
\end{eqnarray*}
m 为数据量， $\alpha$ 为一些递减的固定正值，下确界为0.\\
批量梯度下降的优点：\\
1. 一次迭代对所有样本进行计算，样本操作利用矩阵运算，实现了数据间颚并行操作。\\
2. 容易得到全局最优解。\\
批量梯度下降的缺点：\\
1. 样本容量值 m 巨大时，对全局进行训练，过程缓慢，效率低下.\\
--------------------------------------------------------------------------------------------------\\

\noindent 优化算法底层相关：随机梯度下降（SGD）：选取样本容量中的其中一个值进行梯度下降策略：\\
迭代公式：
\begin{eqnarray*}
\theta_{j+1} = \theta_{j} - \alpha(h_{\theta}(x^{(i)})- y^{i})x^{(i)}_{j}
\end{eqnarray*}
其中:
\begin{eqnarray*}
h_{\theta}(x^{(i)}) = \sum_{j=1}^{n}\theta_{j}x_{j}^{(i)}
\end{eqnarray*}
m 为数据量， $\alpha$ 为一些递减的固定正值，下确界为0.\\
随机梯度下降的优点：\\
1. 一次迭代对单一样本进行计算，效率提升，收敛速度快。\\
随机梯度下降的缺点：\\
1. 准确性下降，$\theta$ 的收敛值依赖于初值的选取，只能得到局部最优解.\\
2. 样本的选取方式导致了样本间不能实现并行计算\\
--------------------------------------------------------------------------------------------------\\

\noindent 优化算法底层相关：小批量梯度下降（MBGD）：选取样本容量中的所有值进行，分组进行梯度下降策略：\\
迭代公式：
\begin{eqnarray*}
\theta_{j+1} = \theta_{j} - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})- y^{i})x^{(i)}_{j}
\end{eqnarray*}
其中:
\begin{eqnarray*}
h_{\theta}(x^{(i)}) = \sum_{j=1}^{n}\theta_{j}x_{j}^{(i)}
\end{eqnarray*}
m 为分组后每组值的样本容量， $\alpha$ 为一些递减的固定正值，下确界为0.\\
小批量梯度下降的优点：\\
1. 每组的样本数据并不多，一次迭代对一组样本进行计算，时间复杂度接近与随机批量下降\\
2. 通过对不同分组间整体数据进行梯度下降，可得到接近与全局最优的解的局部最优解\\
3. 通过分组，数据间实现了整体并行，局部串行\\
小批量梯度下降的缺点：\\
1. 准确性下降，$\theta$ 的收敛值依赖于分组方式.\\
2. 分组过细会导致效率接近于批量梯度下降，分组过粗会导致收敛准确性接近于随机梯度下降\\
--------------------------------------------------------------------------------------------------\\

\noindent 聚类：k-means：以样本距离为主要依据对样本空间进行大致分类，属无监督学习范畴：\\
1. 随机选取样本中的 k 个点（这个是有规则的），记为初始选取点。\\
2. 逐个计算样本点与各个样本中心的距离，寻求离样本最近的样本中心$c_{i}$, 将此样本点划入样本簇 $C_{i}, i=1,2,3,\cdots, k$ 中\\
3. 对于每个簇，重新计算所有点的均值点，列为新的选取点.\\
4. 重复 2 与 3，直到划分的簇 $C_{i},  i=1,2,3,\cdots, k$ 不再变化或产生及其微小的变化\\
--------------------------------------------------------------------------------------------------\\

\noindent 聚类：谱聚类：引入线性代数中的谱，进行分类：\\
1. 计算相似度矩阵 W, 设样本点为：$X=\{x_{1}, x_{2}, \cdots, x_{n}\}$：
\begin{eqnarray*}
W = \sum_{i,j=0}^{n}s_{ij}E_{ij}
\end{eqnarray*}
其中：
\begin{eqnarray*}
s_{ij} = \sum_{i,j=1}^{n}\exp{\frac{-||x_{i}-x_{j}||^{2}}{2\sigma^{2}}}
\end{eqnarray*}
$\sigma$ 为样本的标准差。\\
2. 计算度矩阵D：
\begin{eqnarray*}
D = \sum_{i=1}^{n}d_{i}E_{ii}
\end{eqnarray*}
其中:
\begin{eqnarray*}
d_{i} = \sum_{j=1}^{n}s_{ij}
\end{eqnarray*}
3.1（未标准化的 Laplace 矩阵）计算 Laplace 矩阵 $L=D-W$.\\
3.2（标准化的 Laplace 矩阵）计算 Laplace 矩阵 $L=D^{-1}(D-W)$.\\
3.3（对称化的 Laplace 矩阵）计算 Laplace 矩阵 $L=D^{-\frac{1}{2}}(D-W)D^{-\frac{1}{2}}$.\\
4. 计算矩阵 $L$ 的特征值，并将特征值从小到大，取前面 k 个特征值，并计算这 k 个特征值的特征向量。\\
5. 将这 k 个列向量组成新的矩阵 $U=\sum_{i=1}^{k}u_{i}e_{i}$，其中 $U\in R^{n*k}$。\\
6. 取矩阵 $U$ 的所有行向量作为新的样本中心，以 k-means 算法进行聚类。\\
7. 重复以上步骤，直到样本簇 $C_{i}$ 不再变化或产生极其微小的变化。\\
谱聚类的优点：\\
1. 谱聚类适合样本数量较小的样本集，聚类精确；\\
2. 谱聚类算法使用了降维的技术，所以更加适用于高维数据的聚类；\\
3. 谱聚类只需要数据之间的相似度矩阵，因此对于处理稀疏数据的聚类很有效。这点传统聚类算法（比如K-Means）很难做到。\\
谱聚类的缺点：\\
1. 谱聚类对相似度图的改变和聚类参数的选择非常的敏感；\\
2. 谱聚类依赖于样本点的稀疏度，适用于均衡分类问题，即各簇之间点的个数相差不大，对于簇之间点个数相差悬殊的聚类问题，谱聚类则不适用；\\
3. 谱聚类的计算量十分庞大，不适宜对数据量庞大的样本集进行聚类.\\
--------------------------------------------------------------------------------------------------\\

\noindent 聚类：高斯混合聚类：引入多元高斯分布，以及 EM 算法，进行聚类：\\
1. 首先初始化参数组$(\alpha_{i}, \mu_{i}, \Sigma_{i})$，求各混合成分生成的后验概率：
\begin{eqnarray*}
\gamma_{ij}&=&P_{M}(z_{j}=i|x_{j})\\
&=&\frac{\alpha_{i}\cdot p(x_{i}|\mu_{i},\Sigma_{i})}{\sum_{l=1}^{k}\alpha_{l}\cdot p(x_{i}|\mu_{l},\Sigma_{l})} 
\end{eqnarray*}
其中：
\begin{eqnarray*}
p(x|\mu, \Sigma) = \frac{1}{(2\pi)^{\frac{n}{2}}\cdot |\Sigma|^{\frac{1}{2}}}\exp{(-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu))}
\end{eqnarray*}
$\Sigma$ 是样本 $x=(x_{1}, x_{2}, \cdots, x_{n})$ 的协方差矩阵。\\
2. 更新参数组：
\begin{eqnarray*}
&&\mu_{i}^{'}=\frac{\sum_{j=1}^{m}\gamma_{ji}x_{i}}{\sum_{j=1}^{m}\gamma_{ij}}\\
&&\Sigma_{i}^{'} = \frac{\sum_{j=1}^{m}\gamma_{ij}(x_{j}-\mu_{i}^{'})(x_{j}-\mu_{i}^{'})^{T}}{\sum_{j=1}^{m}\gamma_{ji}}\\
&&\alpha_{i}^{'}=\frac{\sum_{j=1}^{m}\gamma_{ji}}{m}\\
\end{eqnarray*}
3. 将 $x_{j}$ 纳入簇 $C_{\lambda_{j}}$ 中，其中：
\begin{eqnarray*}
\lambda_{j}= argmax_{i\in\{1,2,\cdots, k\}}\gamma_{ji}
\end{eqnarray*}
如此便产生了簇$C_{\lambda_{i}},i=1,2,\cdots, m$\\
4. 重复以上步骤，直到样本簇 $C_{i}$ 不再变化或产生极其微小的变化。\\
--------------------------------------------------------------------------------------------------\\

\noindent 聚类：层次聚类：自下而上，进行聚类：\\
1. 将每个样本点视为一个簇 $C_{i}$ 作为其初始化.\\
2. 制作样本点矩阵 M，其中：
\begin{eqnarray*}
&&M = \sum_{i,j=1}^{m}d(C_{i}, C_{j})E_{ij}\\
&&d(C_{i}, C_{j}) = 
\begin{cases}
d_{min}(C_{i}, C_{j}) = \min_{x\in C_{i}, z\in C_{j}} d(x, z), & \text{单链接聚类}\\
d_{max}(C_{i}, C_{j}) = \max_{x\in C_{i}, z\in C_{j}} d(x, z), & \text{全链接聚类}\\
d_{avg}(C_{i}, C_{j}) = \frac{1}{|C_{i}|*|C_{j}|}\sum_{x\in C_{i}, z\in C_{j}}d(x, z), & \text{均链接聚类}\\
\end{cases}
\end{eqnarray*}
3. 设目标聚类个数为 k：重复下述步骤，直到 $q\leq k$：\\
3.1 寻找距离最近的两个簇 $C_{i^{*}}, C_{j^{*}}$, 并合并：
\begin{eqnarray*}
C_{i^{*}} = C_{i^{*}}\cup C_{j^{*}}
\end{eqnarray*}
3.2 将编号为 $j^{*} + 1$ 到 $q$ 的簇重新编号为 $j^{*}$ 到 $q-1$。\\
3.3 删除矩阵 M 的第 $j^{*}$ 行与第 $j^{*}$ 列。\\
3.4 更新矩阵 M：
\begin{eqnarray*}
&&M(i^{*}, j) = d(C_{i^{*}}, C_{j})\\
&&M(j, i^{*}) = M(i^{*}, j)
\end{eqnarray*}
3.5 $q = q-1$\\
--------------------------------------------------------------------------------------------------\\

\noindent 分布相关：首先介绍对于概率密度函数，期望，方差及特征函数的求解方法：
\begin{eqnarray*}
&&E[x] = \int_{A}^{B}x*p(x)dx\\
&&var[x] = \int_{A}^{B}(x-E[x])^{2}*p(x)dx\\
&&E(e^{itx})=\int_{-\infty}^{\infty}e^{itx}P(x)dx
\end{eqnarray*}
--------------------------------------------------------------------------------------------------\\

\noindent 分布相关：均匀分布(a, b 为函数有图像的两个横坐标)：\\
\begin{eqnarray*}
&&p(x|a,b) = \frac{1}{b-a}\\
&&E[x] = \frac{b+a}{2}\\
&&var[x] = \frac{(b-a)^{2}}{12}
\end{eqnarray*}
--------------------------------------------------------------------------------------------------\\

\noindent 分布相关：伯努利分布($p(x=1) = \mu$)：\\
\begin{eqnarray*}
&&P(x|\mu) = \mu^{x}(1-\mu)^{1-x}\\
&&E[x] = \mu\\
&&var[x] = \mu(1-\mu)
\end{eqnarray*}
--------------------------------------------------------------------------------------------------\\

\noindent 分布相关：二项分布（例如抛硬币，成功的概率为 $\mu$）：\\
\begin{eqnarray*}
&&P(m|N.\mu) = C_{N}^{m}\mu^{m}(1-\mu)^{N-m}\\
&&E[x] = N\mu\\
&&var[x] = N\mu(1-\mu)
\end{eqnarray*}
--------------------------------------------------------------------------------------------------\\

\noindent 分布相关：负二项分布(pascal分布)\\
\begin{eqnarray*}
P(k;r,p)= C_{k+r-1}^{r-1}p^{r}(1-p)^{k}
\end{eqnarray*}
其中，$p$ 表示在事件在伯努利实验中的单次成功概率，并进行了 $r+k$ 次实验，成功 $r$ 次。\\
并且:
\begin{eqnarray*}
C_{k+r-1}^{k} = (-1)^{k}C_{-r}^{k}
\end{eqnarray*}
补充：\\
1. 当组合数底数由自然数域扩充到实数域：有：
\begin{eqnarray*}
C_{-n}^{m}= \frac{-n(-n-1)\cdots(-n-k+1)}{m!}
\end{eqnarray*}
2. 当组合数系数全部位置由自然数域扩充到实数域：有：
\begin{eqnarray*}
C_{a}^{b} = \frac{\Gamma(a+1)}{\Gamma(b+1)\Gamma(a-b+1)}
\end{eqnarray*}
--------------------------------------------------------------------------------------------------\\

\noindent 分布相关：多项分布(N次独立实验)：\\
\begin{eqnarray*}
&&P(m|N.\mu) = \frac{N!}{\prod_{i=1}^{d}m_{i}!}\prod_{i=1}^{d}\mu_{i}^{m_{i}}\\
&&E[x_{i}] = N\mu_{i}\\
&&var[x_{i}] = N\mu_{i}(1-\mu_{i})
\end{eqnarray*}
--------------------------------------------------------------------------------------------------\\

\noindent 分布相关：$\beta$ 分布：\\
\begin{eqnarray*}
&&P(\mu|a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\\
&&E[\mu] = \frac{a}{a+b}\\
&&var[\mu] = \frac{ab}{(a+b)^{2}*(a+b+1)}
\end{eqnarray*}
其中: $\Gamma$ 函数：
\begin{eqnarray*}
\Gamma(a) = \int_{0}^{+\infty}t^{a-1}e^{-t}dt
\end{eqnarray*}
--------------------------------------------------------------------------------------------------\\

\noindent 分布相关：$Gamma$ 分布：\\
\begin{eqnarray*}
&&P(x|\beta, \alpha)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}\\
&&E[x] = \frac{\alpha}{\beta}\\
&&var[x] = \frac{\alpha}{\beta^{2}}
\end{eqnarray*}
其中, $\alpha$ 为等候时间, $\beta$ 是定值。	\\
--------------------------------------------------------------------------------------------------\\

\noindent 分布相关：Dirichlet 分布：\\
令 $\bar{\alpha}=\sum_{i=1}^{d}\alpha_{i}$, 则：
\begin{eqnarray*}
&&P(\mu_{1},\cdots, \mu_{d}|\alpha_{1},\cdots, \alpha_{d}) = \frac{\Gamma(\bar{\alpha})}{\prod_{i=1}^{d}\Gamma(\alpha_{i})}\prod_{i=1}^{d}\mu_{i}^{\alpha_{i}-1}\\
&&E[\mu_{i}] = \frac{\alpha_{i}}{\bar{\alpha}}\\
&&var[\mu_{i}] = \frac{\alpha_{i}(\bar{\alpha}-\alpha_{i})}{\bar{\alpha}^{2}(\bar{\alpha} + 1)}\\
\end{eqnarray*}
--------------------------------------------------------------------------------------------------\\

\noindent 分布相关：高斯分布(多维)：\\
\begin{eqnarray*}
&&p(\bar{x}|\mu, \Sigma) = \frac{1}{(2\pi)^{\frac{d}{2}}|\Sigma|^{\frac{1}{2}}}\exp{(-\frac{1}{2}(\bar{x}-\mu)^{T}\Sigma^{-1}(\bar{x}-\mu))}\\
&&E[x]=\mu\\
&&cov[x]=\Sigma
\end{eqnarray*}
--------------------------------------------------------------------------------------------------\\

\noindent 分布相关：possion 分布：\\
\begin{eqnarray*}
&&P(x=k)=\frac{\lambda^{k}}{k!}e^{-\lambda}\\
&&E[x]=\lambda\\
&&var[x]=\lambda\\
\end{eqnarray*}
--------------------------------------------------------------------------------------------------\\

\noindent 分布相关：超几何分布：\\
\begin{eqnarray*}
&&P(x=k) = \frac{C_{M}^{k}C_{N-M}^{n-k}}{C_{N}^{n}}\\
&&E[x] = \frac{nM}{N}\\
&&var[x] = \frac{nNM-n^{2}M^{2}}{N^{2}}+\frac{n(n-1)M(M-1)}{N(N-1)}
\end{eqnarray*}
其中，单次成功的概率为 $\frac{M}{N}$，N为样本总数，M为特征样本。\\
--------------------------------------------------------------------------------------------------\\

\noindent 分布相关：t分布：\\
\begin{eqnarray*}
&&P(x|n) = \frac{\Gamma(\frac{n+1}{2})}{(n\pi)^{\frac{1}{2}}\Gamma(\frac{n}{2})}(1+\frac{x^{2}}{n})^{-\frac{n+1}{2}}\\
&&E[x] = 0\\
&&var[x]=\frac{n}{n-2}
\end{eqnarray*}
其中，$\beta$函数 B：
\begin{eqnarray*}
B(m,n) = \frac{\Gamma(m)\Gamma(n)}{\Gamma(m+n)}
\end{eqnarray*}
--------------------------------------------------------------------------------------------------\\

\noindent 分布相关：F分布：\\
\begin{eqnarray*}
&&P(x|n,m)=\frac{(\frac{n}{m})^{\frac{n}{2}}}{B(\frac{n}{2}, \frac{m}{2})}x^{\frac{n}{2}-1}(1+\frac{n}{m}x)^{-\frac{n+m}{2}}\\
&&E[x]=\frac{n}{n-2}\\
&&var[x]=\frac{2n^{2(m+n-2)}}{m(n-2)^{2(n-4)}}
\end{eqnarray*}
--------------------------------------------------------------------------------------------------\\

\noindent 分布相关：卡方分布($\sigma$分布)：\\
\begin{eqnarray*}
&&P(x|n)=\frac{1}{2^{\frac{n}{2}}\Gamma(\frac{n}{2})}x^{\frac{n}{2}-1}e^{-\frac{x}{2}}\\
&&E[x] = n\\
&&var[x]=2n\\
\end{eqnarray*}
--------------------------------------------------------------------------------------------------\\

\noindent 集成：adaboost：\\
1. 初始化权值分布:
\begin{eqnarray*}
D_{1} = \{w_{1,1}, \cdots, w_{1,N}\},\,\,\, w_{1,i} = \frac{1}{N},\,\,\, i = 1,2,\cdots, N
\end{eqnarray*}
2. 设置迭代次数为 M，对于每一步迭代 $m=1,2,\cdots, M$:	\\
2.1 使用权值分布 $D_{m}$ 的训练数据集，进行学习，得到弱分类器 $G_{m}$\\
2.2 计算分类误差率：
\begin{eqnarray*}
e_{m}=\sum_{i=1}^{N}w_{m,i}I(G_{m}(x_{i})\ne y_{i})
\end{eqnarray*}
2.3 计算$G_{m}(x)$ 在强分类器中的所占的权重：
\begin{eqnarray*}
\alpha_{m} = \frac{1}{2}\log{(\frac{1-e_{m}}{e_{m}})}
\end{eqnarray*}
2.4 更新权值分布：
\begin{eqnarray*}
w_{m+1, i}= \frac{w_{m,i}}{z_{m}}\exp{(-\alpha_{m}y_{i}G_{m}(x_{i}))}
\end{eqnarray*}
其中:
\begin{eqnarray*}
z_{m} = \sum_{i=1}^{N}w_{m,i}\exp{(-\alpha_{m}y_{i}G_{m}(x_{i}))}
\end{eqnarray*}
3. 得到分类器：
\begin{eqnarray*}
F(x) = sign(\sum_{i=1}^{N}\alpha_{m}G_{m}(x))
\end{eqnarray*}







\end{document}